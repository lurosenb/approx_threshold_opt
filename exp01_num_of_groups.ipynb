{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import yaml\n",
    "import argparse\n",
    "import gc\n",
    "\n",
    "from metrics import tpr, fpr, precision, npv, accuracy, f1, selection_rate\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from pipeline import FairDataset, FairPipeline, tpr_score, fpr_score, npv_score, selection_rate_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_classifiers = {\n",
    "    # 'lgb': lgb.LGBMClassifier(verbose=-1)\n",
    "    'xgb': xgb.XGBClassifier()\n",
    "    # 'logistic_regression': LogisticRegression(),\n",
    "    # 'random_forest': RandomForestClassifier(),\n",
    "    # 'gradient_boosting': GradientBoostingClassifier(),\n",
    "    # 'svc': SVC(probability=True),\n",
    "    # 'knn': KNeighborsClassifier(),\n",
    "    # 'mlp': MLPClassifier()\n",
    "    # decision tree\n",
    "}\n",
    "\n",
    "# These are the metrics included in the results dict\n",
    "metrics_dict = {\n",
    "    'tpr': tpr_score,\n",
    "    'fpr': fpr_score,\n",
    "    'precision': precision_score,\n",
    "    'npv': npv_score,\n",
    "    'accuracy': accuracy_score,\n",
    "    'f1': f1_score,\n",
    "    'selection_rate': selection_rate_score\n",
    "}\n",
    "\n",
    "# This is what we are equalizing in the objective\n",
    "metrics_functions = {\n",
    "    'tpr': tpr,\n",
    "    'fpr': fpr,\n",
    "    'precision': precision,\n",
    "    'npv': npv,\n",
    "    'selection_rate': selection_rate\n",
    "}\n",
    "\n",
    "global_metrics_map = {\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'npv': npv,\n",
    "    'accuracy': accuracy,\n",
    "}\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "#parser = argparse.ArgumentParser(description='Run the pipeline for a specific dataset and with a custom output file name.')\n",
    "#parser.add_argument('--dataset_name', type=str, help='Name of the dataset to run the pipeline for', required=True)\n",
    "#parser.add_argument('--output_file', type=str, help='Name for the output .pkl file', required=True)\n",
    "\n",
    "#args = parser.parse_args()\n",
    "\n",
    "CONFIG_PATH = 'configs/exp01_num_of_groups.yml'\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "datasets = config['datasets']\n",
    "dataset_names = list(datasets.keys())\n",
    "datasets_settings = config['datasets_settings']\n",
    "lambda_settings = config['lambda_settings']\n",
    "\n",
    "classifier_config_path = 'configs/test_classifier_config.yml'\n",
    "\n",
    "#DATASET_NAME = args.dataset_name\n",
    "OUTPUT_FILE = 'exp01_results.pkl'\n",
    "\n",
    "#print(DATASET_NAME)\n",
    "#print(OUTPUT_FILE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set to True to estimate runtime\n",
    "ESTIMATE_RUNTIME = False\n",
    "\n",
    "  \n",
    "all_results = pd.DataFrame()\n",
    "for DATASET_NAME in dataset_names:\n",
    "    print(DATASET_NAME)\n",
    "\n",
    "    sensitive_attrs = datasets[DATASET_NAME]\n",
    "\n",
    "    sensitive_attr = sensitive_attrs[0] # This is only for this expeirment, we don't need\n",
    "                                        #   a list of sensitive attributes since we are focusing\n",
    "                                        #   on RAC1P_Recoded\n",
    "\n",
    "    global_metric_setting = datasets_settings[DATASET_NAME][0]\n",
    "    lambda_list = lambda_settings[DATASET_NAME]\n",
    "\n",
    "    print(f\"Running pipeline for dataset: {DATASET_NAME}\")\n",
    "    if DATASET_NAME in ('ACSEmployment','ACSIncome','ACSMobility','ACSPublicCoverage','ACSTravelTime'):\n",
    "        X = pd.read_csv(f'matrices/{DATASET_NAME}/Xs.csv')\n",
    "        y = pd.read_csv(f'matrices/{DATASET_NAME}/ys.csv').squeeze()\n",
    "    else:\n",
    "        X = pd.read_csv(f'matrices/{DATASET_NAME}/X.csv')\n",
    "        y = pd.read_csv(f'matrices/{DATASET_NAME}/y.csv').squeeze()\n",
    "\n",
    "    X_raw = X.copy()\n",
    "    num_of_groups_list = [  # Sets of groups for which the experiment will run\n",
    "                          [1,2], # White, Black\n",
    "                          [1,2,3], # White, Black, Asian\n",
    "                          [1,2,3,4] # White, Black, Asain, Other\n",
    "                          ]\n",
    "\n",
    "    for groups in num_of_groups_list:\n",
    "        X = X_raw.loc[X_raw[sensitive_attr].isin(groups)]\n",
    "\n",
    "        # remove any rows that have null or nan\n",
    "        X.dropna(inplace=True)\n",
    "        \n",
    "        # Select appropiate y rows\n",
    "        y = y[X.index]\n",
    "\n",
    "    \n",
    "\n",
    "        dataset = FairDataset(X, y, sensitive_attrs)\n",
    "\n",
    "        pipeline = FairPipeline(classifiers=my_classifiers, \n",
    "                                classifier_config_path=classifier_config_path, \n",
    "                                metrics=metrics_dict,\n",
    "                                metric_functions=metrics_functions,\n",
    "                                global_metric=global_metrics_map[global_metric_setting],\n",
    "                                lambdas=lambda_list,\n",
    "                                max_error=0.01, max_total_combinations=50000)\n",
    "\n",
    "        pipeline.tune_and_evaluate(dataset, DATASET_NAME, sensitive_attr)\n",
    "        results = pipeline.results_df\n",
    "        results['sensitive_attr'] = sensitive_attr\n",
    "        results['dataset'] = DATASET_NAME\n",
    "        groups_str = ','.join(str(g) for g in groups)\n",
    "        results['groups_included'] = groups_str\n",
    "        results['num_groups_included'] = max(groups)\n",
    "        # all_results = all_results.append(results, ignore_index=True)\n",
    "        all_results = pd.concat([all_results, results], ignore_index=True)\n",
    "        print()\n",
    "        print('Overall max_error')\n",
    "        print(pipeline.overall_max_error)\n",
    "        print()\n",
    "\n",
    "        # this avoids memory issues\n",
    "        del X, y, dataset\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Pipeline run completed.\")\n",
    "\n",
    "        all_results.to_pickle(OUTPUT_FILE)\n",
    "\n",
    "        break\n",
    "    break\n",
    "else:\n",
    "    print(f\"Dataset {DATASET_NAME} not found in the configuration.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_results.head()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}